{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a7fb8991-26fd-485f-bdea-db4210cb86da",
   "metadata": {},
   "source": [
    "### Adaptive Fault Detection with LSTM Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdd88b48-45f7-4061-b2a1-796ccc1b4a7a",
   "metadata": {},
   "source": [
    "##### Import libraries needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "730016e5-cf69-49fc-840d-2a7b8849a5c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import joblib\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.layers import LSTM, Dense, RepeatVector, TimeDistributed\n",
    "import pickle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9086f5cd-f016-4349-9f9f-97a11e1e2ed7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "623/623 - 34s - loss: 0.0291 - 34s/epoch - 55ms/step\n",
      "Epoch 2/10\n",
      "623/623 - 30s - loss: 0.0154 - 30s/epoch - 47ms/step\n",
      "Epoch 3/10\n",
      "623/623 - 30s - loss: 0.0143 - 30s/epoch - 48ms/step\n",
      "Epoch 4/10\n",
      "623/623 - 30s - loss: 0.0136 - 30s/epoch - 48ms/step\n",
      "Epoch 5/10\n",
      "623/623 - 30s - loss: 0.0132 - 30s/epoch - 48ms/step\n",
      "Epoch 6/10\n",
      "623/623 - 30s - loss: 0.0130 - 30s/epoch - 47ms/step\n",
      "Epoch 7/10\n",
      "623/623 - 30s - loss: 0.0127 - 30s/epoch - 48ms/step\n",
      "Epoch 8/10\n",
      "623/623 - 31s - loss: 0.0125 - 31s/epoch - 50ms/step\n",
      "Epoch 9/10\n",
      "623/623 - 34s - loss: 0.0123 - 34s/epoch - 54ms/step\n",
      "Epoch 10/10\n",
      "623/623 - 30s - loss: 0.0123 - 30s/epoch - 48ms/step\n",
      "0.013408180326223373\n"
     ]
    }
   ],
   "source": [
    "# Data Preparation\n",
    "# def prepare_data(data, n_steps_in, n_steps_out):\n",
    "#     X, y = [], []\n",
    "#     for i in range(len(data)):\n",
    "#         end_ix = i + n_steps_in\n",
    "#         out_end_ix = end_ix + n_steps_out - 1\n",
    "#         if out_end_ix > len(data) - 1:\n",
    "#             break\n",
    "#         # seq_x, seq_y = data[i:end_ix, :], data[end_ix-1:out_end_ix, :]\n",
    "#         seq_x, seq_y = data[i:end_ix, :], data[end_ix:out_end_ix, :]\n",
    "#         X.append(seq_x)\n",
    "#         y.append(seq_y)\n",
    "#     return np.array(X), np.array(y)\n",
    "\n",
    "def prepare_data(data, n_steps_in, n_steps_out):\n",
    "    X, y = [], []\n",
    "    for i in range(len(data)):\n",
    "        end_ix = i + n_steps_in\n",
    "        out_end_ix = end_ix + n_steps_out\n",
    "        if out_end_ix > len(data):\n",
    "            break\n",
    "        seq_x, seq_y = data[i:end_ix, :], data[end_ix:out_end_ix, :]\n",
    "        X.append(seq_x)\n",
    "        y.append(seq_y)\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "def generate_random_name(length):\n",
    "    # random_string = ''.join(random.choices(string.ascii_letters + string.digits, k=length))\n",
    "    random_string = ''.join(random.choices(string.ascii_letters, k=length))\n",
    "    # return \"expert_models/New_expert_model_\" + random_string + \".pkl\"\n",
    "    return \"new_lstm_expert_model_\" + random_string + \".pkl\"\n",
    "\n",
    "\n",
    "\n",
    "# Load your data\n",
    "# data = pd.read_csv('your_data.csv').values\n",
    "# For example, let's create dummy data\n",
    "# data = np.random.rand(1000, 3)\n",
    "# file_name = 'waterTank_Golden_reduced.csv'\n",
    "file_name = 'stuckat0_training_reduced_new.csv'\n",
    "df = pd.read_csv(file_name, header=0, index_col=0).values\n",
    "\n",
    "\n",
    "# Normalize the data\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "data = scaler.fit_transform(df)\n",
    "\n",
    "# Prepare data for LSTM\n",
    "n_steps_in, n_steps_out = 50, 30\n",
    "X, y = prepare_data(data, n_steps_in, n_steps_out)\n",
    "\n",
    "# Define the model\n",
    "model = Sequential()\n",
    "model.add(LSTM(50, activation='relu', input_shape=(n_steps_in, X.shape[2])))\n",
    "model.add(RepeatVector(n_steps_out))\n",
    "model.add(LSTM(50, activation='relu', return_sequences=True))\n",
    "model.add(TimeDistributed(Dense(X.shape[2])))\n",
    "# model.add(Dense(y.shape[2]))\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "# Fit the model\n",
    "# model.fit(X, y, epochs=300, verbose=1)\n",
    "train_history = model.fit(X, y, epochs=10, batch_size=32, verbose=2)\n",
    "\n",
    "# Evaluate the model\n",
    "mse = model.evaluate(X, y, verbose=0)\n",
    "print(mse)\n",
    "\n",
    "# Save the model and scaler\n",
    "model.save('expert_models_lstm/lstm_model.h5')\n",
    "with open('expert_models_lstm/scaler.pkl', 'wb') as f:\n",
    "    pickle.dump(scaler, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca86093b-45f7-4f63-baed-8e3563be17a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ef47d96d-51e0-4bab-a4cd-7de2c37f4ee3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[ 1.50678307e-01  2.37167869e-02  9.75447614e-03  4.54175845e-02\n",
      "    8.20439681e-03]\n",
      "  [ 1.21236153e-01  6.06210902e-03  9.72293597e-03  2.30790041e-02\n",
      "    9.77439061e-03]\n",
      "  [ 9.81832743e-02 -2.68591568e-03  8.84400215e-03  9.86955315e-03\n",
      "    1.17681213e-02]\n",
      "  ...\n",
      "  [ 1.68199435e-01  3.92675027e-03 -2.36986298e-03  1.73154846e-02\n",
      "    1.18124858e-03]\n",
      "  [ 1.68046072e-01  3.90850380e-03 -2.37983372e-03  1.73333436e-02\n",
      "    1.18910521e-03]\n",
      "  [ 1.67878956e-01  3.87736782e-03 -2.38800328e-03  1.73477083e-02\n",
      "    1.19851902e-03]]\n",
      "\n",
      " [[ 1.51716828e-01  2.36505009e-02  9.67774075e-03  4.56744507e-02\n",
      "    8.01502727e-03]\n",
      "  [ 1.22508258e-01  6.00035861e-03  9.51177999e-03  2.36011967e-02\n",
      "    9.46087018e-03]\n",
      "  [ 9.95801166e-02 -2.85465643e-03  8.62168241e-03  1.03635825e-02\n",
      "    1.15375854e-02]\n",
      "  ...\n",
      "  [ 1.69664606e-01  3.81430611e-03 -2.34750938e-03  1.73534453e-02\n",
      "    1.45225599e-03]\n",
      "  [ 1.69524997e-01  3.80128250e-03 -2.35731993e-03  1.73716955e-02\n",
      "    1.45931914e-03]\n",
      "  [ 1.69370472e-01  3.77504900e-03 -2.36534141e-03  1.73862875e-02\n",
      "    1.46798044e-03]]\n",
      "\n",
      " [[ 1.52785748e-01  2.35988423e-02  9.59380250e-03  4.59252857e-02\n",
      "    7.82827102e-03]\n",
      "  [ 1.23805434e-01  5.95920533e-03  9.29449964e-03  2.41186656e-02\n",
      "    9.14758630e-03]\n",
      "  [ 1.01002082e-01 -2.99219415e-03  8.39642994e-03  1.08550750e-02\n",
      "    1.12977065e-02]\n",
      "  ...\n",
      "  [ 1.71167642e-01  3.76476720e-03 -2.32240092e-03  1.73907727e-02\n",
      "    1.69527531e-03]\n",
      "  [ 1.71041489e-01  3.75684723e-03 -2.33198050e-03  1.74093321e-02\n",
      "    1.70154497e-03]\n",
      "  [ 1.70899361e-01  3.73537466e-03 -2.33981386e-03  1.74241364e-02\n",
      "    1.70942023e-03]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[ 1.04465175e+00  9.77207184e-01  3.29961888e-02  8.98525417e-02\n",
      "    5.05175292e-01]\n",
      "  [ 9.54505384e-01  9.96896446e-01  1.39968740e-02  1.18096769e-01\n",
      "    2.06941530e-01]\n",
      "  [ 1.00136888e+00  1.00273550e+00  1.08715845e-02  1.32651120e-01\n",
      "    5.34556620e-02]\n",
      "  ...\n",
      "  [ 1.01132131e+00  9.77102757e-01 -3.34317330e-03  1.38887495e-01\n",
      "   -1.33806206e-02]\n",
      "  [ 1.01125240e+00  9.77011025e-01 -3.34618706e-03  1.39020354e-01\n",
      "   -1.30451880e-02]\n",
      "  [ 1.01118219e+00  9.76928294e-01 -3.35016940e-03  1.39137357e-01\n",
      "   -1.27389468e-02]]\n",
      "\n",
      " [[ 1.00975585e+00  9.92632806e-01 -2.26680450e-02  1.19041622e-01\n",
      "    3.37517969e-02]\n",
      "  [ 1.01435399e+00  9.88209903e-01 -7.24271871e-04  1.21650368e-01\n",
      "   -6.76735118e-03]\n",
      "  [ 1.03210330e+00  9.64145839e-01  6.34299498e-03  1.26618415e-01\n",
      "   -2.23310404e-02]\n",
      "  ...\n",
      "  [ 1.01565289e+00  9.79522228e-01 -1.40471663e-03  1.46815777e-01\n",
      "   -2.18840130e-02]\n",
      "  [ 1.01564229e+00  9.79434431e-01 -1.41129550e-03  1.46875307e-01\n",
      "   -2.17370205e-02]\n",
      "  [ 1.01562798e+00  9.79355991e-01 -1.41799729e-03  1.46925211e-01\n",
      "   -2.16047131e-02]]\n",
      "\n",
      " [[ 1.00555515e+00  9.71760094e-01 -2.91012861e-02  1.34399801e-01\n",
      "   -4.10691239e-02]\n",
      "  [ 1.02166414e+00  9.76055741e-01 -5.61925583e-04  1.25260800e-01\n",
      "   -3.34013291e-02]\n",
      "  [ 1.02991927e+00  9.59135830e-01  6.52593840e-03  1.29415303e-01\n",
      "   -2.71478631e-02]\n",
      "  ...\n",
      "  [ 1.01750326e+00  9.73424196e-01 -3.80812120e-03  1.53430730e-01\n",
      "   -2.44432129e-02]\n",
      "  [ 1.01749587e+00  9.73350585e-01 -3.81564628e-03  1.53485745e-01\n",
      "   -2.43159942e-02]\n",
      "  [ 1.01748514e+00  9.73285377e-01 -3.82300373e-03  1.53531849e-01\n",
      "   -2.42025591e-02]]]\n"
     ]
    }
   ],
   "source": [
    "def load_model_and_predict(new_data, n_steps_in, n_steps_out):\n",
    "    model = load_model('expert_models_lstm/lstm_model.h5')\n",
    "    with open('expert_models_lstm/scaler.pkl', 'rb') as f:\n",
    "        scaler = pickle.load(f)\n",
    "\n",
    "    # Normalize new data\n",
    "    new_data = scaler.transform(new_data)\n",
    "\n",
    "    # Prepare input data\n",
    "    X_new = []\n",
    "    for i in range(len(new_data) - n_steps_in + 1):\n",
    "        X_new.append(new_data[i:i + n_steps_in, :])\n",
    "    X_new = np.array(X_new)\n",
    "\n",
    "    # Predict\n",
    "    # y_pred = model.predict(X_new)\n",
    "    y_pred = model.predict(X_new)\n",
    "    \n",
    "    # Inverse transform predictions\n",
    "    # y_predic = scaler.inverse_transform(y_pred)\n",
    "    # return y_predic\n",
    "    return y_pred\n",
    "\n",
    "# Example of using the function with new data\n",
    "new_file_name = 'stuckat0_training_reduced_new.csv'\n",
    "new_data = pd.read_csv(new_file_name, header=0, index_col=0).values\n",
    "\n",
    "\n",
    "# new_data = np.random.rand(15, 3)\n",
    "P, q = prepare_data(new_data, n_steps_in, n_steps_out)\n",
    "# predictions = load_model_and_predict(P, n_steps_in, n_steps_out)\n",
    "predictions = load_model_and_predict(new_data, n_steps_in, n_steps_out)\n",
    "print(predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "65d9f976-1c7d-4760-afd0-35759db2175f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20001, 5)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5a3c16b3-6359-49b8-851b-a64b91e69a58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19952, 30, 5)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6f21c681-83a0-4463-a45b-d2e57e143337",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19922, 30, 5)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f51f1c7-5f78-4ebf-a812-39fe5a7263f1",
   "metadata": {},
   "source": [
    "#### LSTM Model Training and Saving:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbf2e4d1-52fa-41a1-9a4b-78c141e351cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = 'waterTank_Golden_reduced.csv'\n",
    "data = pd.read_csv(file_name)  # Replace with your actual data source\n",
    "values = data['value'].values.reshape(-1, 1)\n",
    "# Normalize the data\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "scaled_values = scaler.fit_transform(values)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "083f1511-d66a-4632-89a3-22bd82b2e70a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e9f4c1c2-12a8-470a-b85e-1dd0c0a26458",
   "metadata": {},
   "source": [
    "#### Load the Model and Predict New Values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5dcbefb-4aab-4d57-ae19-8ebb65d8f1e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model and scaler\n",
    "model = load_model('lstm_model.h5')\n",
    "scaler = joblib.load('scaler.pkl')\n",
    "\n",
    "# Function to predict the next n values\n",
    "def predict_next_n_values(data, n=1, look_back=10):\n",
    "    # Ensure data is scaled using the same scaler\n",
    "    scaled_data = scaler.transform(data)\n",
    "\n",
    "    # Prepare the data for prediction\n",
    "    X = []\n",
    "    for i in range(len(scaled_data) - look_back):\n",
    "        X.append(scaled_data[i:(i + look_back), 0])\n",
    "    X = np.array(X)\n",
    "    X = np.reshape(X, (X.shape[0], X.shape[1], 1))\n",
    "\n",
    "    # Predict the next values\n",
    "    predictions = []\n",
    "    input_seq = X[-1]\n",
    "    for _ in range(n):\n",
    "        pred = model.predict(np.array([input_seq]))[0][0]\n",
    "        predictions.append(pred)\n",
    "        input_seq = np.append(input_seq[1:], [[pred]], axis=0)\n",
    "\n",
    "    # Inverse transform the predictions\n",
    "    predictions = scaler.inverse_transform(np.array(predictions).reshape(-1, 1))\n",
    "    return predictions\n",
    "\n",
    "# Example usage\n",
    "# Load new data for prediction\n",
    "new_data = pd.read_csv('new_time_series_data.csv')  # Replace with your actual new data source\n",
    "new_values = new_data['value'].values.reshape(-1, 1)\n",
    "\n",
    "# Predict the next 5 values\n",
    "next_5_values = predict_next_n_values(new_values, n=5)\n",
    "print(next_5_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37408e1c-6c4d-4d76-92a7-107d6a9b8885",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02d76c7d-830f-4f05-a729-306d34fe5020",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa8fb6ee-9791-437c-af94-553b84dc8335",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efffe3cf-9f03-4034-b59d-c51467722871",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01c17e77-e2e2-41fe-a6c8-ae7eb99ca1c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58762dab-5ebf-4ab4-bca1-1ca97ace62ef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
